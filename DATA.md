# Data Artifacts
[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.18611335.svg)](https://doi.org/10.5281/zenodo.18611335) \
**The experimental data artifacts and model checkpoints for our study are archived on [Zenodo](https://doi.org/10.5281/zenodo.18611335).**

Our published data artifacts include:
- **Model Checkpoints:** Weights for the reproduced QDER models.
- **Experimental Results:** Raw run-files and evaluation metrics for all configurations discussed in the paper.

## Data Organization
The project data is organized into three primary folders: `Data`, `Models`, and `Rankings`.

### Data Artifacts
We provide the following data artifacts used in our reproduction:
1. `bm25.entity_ranking.txt`: The entity ranking generated by the heuristic aggregation method, based on BM25+RM3 (*sec. 4.2*).
2. `entity-links.robust04.jsonl`: Entity links for [TREC Robust 2004](https://trec.nist.gov/data/t13_robust.html), created using the [WAT entity annotator](https://sobigdata.d4science.org/web/tagme/wat-api).
3. `folds.json`: Generated fold (boundary) instructions, used to train and evaluate all models.
4. `title.bm25-rm3.run`: Initial BM25+RM3 document ranking using [Pyserini](https://github.com/castorini/pyserini).

We exclude the following data artifacts, as these materials are publicly available from their respective sources:

5. `mmead_embeddings.jsonl`: Pre-trained [Wikipedia2Vec](https://wikipedia2vec.github.io/wikipedia2vec/) entity embeddings, sourced from [MMEAD](https://github.com/informagi/mmead).
6. `title.queries.tsv`: The set of search queries (topics) for the Robust04 track.
7. `qrels.robust04.txt`: The standard TREC query relevance assessments (QRELs) for the Robust04 track.
8. `entity_descriptions.jsonl`: English entity descriptions, taken from the [DBpedia 2016.10.01](https://downloads.dbpedia.org/repo/lts/text/short-abstracts/2016.10.01/) short abstracts (`short-abstracts_lang=en`).

#### Structure
```bash
Data
├── bm25.entity_ranking.txt
├── entity-links.robust04.jsonl
├── folds.json
└── title.bm25-rm3.run
```

### Final Rankings
To facilitate comprehensive evaluation, we provide the full output of our ranking pipelines.
All model rankings and results are partitioned across `5-fold cross-validation`, where each model configuration (e.g., Balanced vs. Unbalanced) contains a `.run` file corresponding to the folds defined in `folds.json`.

We use the following naming conventions to organize the results into three primary folders:

- **Document_Ranker**: Contains the final document re-rankings.
- **Entity_Ranker**: Contains the query-entity rankings produced by the **BERT-based neural entity ranker** used as input for the dual-model QDER architecture.
  (Note: The heuristic-based entity rankings are provided in the **Data** directory).
- **Configurations**: Aggregated rankings for each configuration, these are our reported rankings.

Naming conventions:
- **BM25**: Indicates the entity ranking used is based on the **heuristic aggregation method** (*eq. 8*), which transfers document scores from the initial retrieval (BM25+RM3) to entities.
- **NEUR**: Indicates the entity ranking used is produced by the **BERT-based neural entity ranker** (*sec. 3.5*).\
  (e.g., all **Biased** configurations are based on the `filtered.entity_ranking.run` ranking)
- **All-Interactions**: Implementation uses all three embeddings interactions (Addition, Multiplication, Subtraction).
- **No-Subtract**: Implementation follows the described model architecture; excluding the Subtraction interaction.
- **Biased**: Implementation uses the **Filtered** entity ranking, which incorporates ground-truth relevance to filter the entity pool (*sec. 3.5.3*).
- **Unbiased**: Implementation uses the **Complete** entity ranking, reflecting the realistic entity pool without ground-truth filtering (*sec. 3.6*).
- **Balanced/Unbalanced**: Refers to the usage of the positive/negative class-balancing method on the evaluation candidates (*sec. 3.1.4*).
- **Min-Max**: Refers to the *min-max normalized* variant of the encompassing folder.

_(Note: The **Balanced** and **Biased** rankings are influenced by external document-level relevance signals, not available in a realistic retrieval scenario)._

#### Structure

```bash
Rankings
├── Configurations
│   ├── config-1.run
│   ├── config-2.run
│   ├── config-3.run
│   ├── config-4.run
│   ├── config-5.run
│   └── config-6.run
├── Document_Ranker
│   ├── BM25_AllInteractions_Balanced          # Config. #3 (Heuristic + All Interactions)
│   │   ├── qder.fold-0.balanced.run
│   │   ├── qder.fold-1.balanced.run
│   │   ├── qder.fold-2.balanced.run
│   │   ├── qder.fold-3.balanced.run
│   │   └── qder.fold-4.balanced.run
│   ├── BM25_AllInteractions_Unbalanced        # Config. #1
│   ├── BM25_NoSubtract_Balanced               # Config. #4
│   ├── BM25_NoSubtract_Unbalanced             # Config. #2
│   ├── NEUR_AllInteractions_Biased
│   ├── NEUR_NoSubtract_Biased
│   │   └── Min-Max                            # Config. #5
│   └── NEUR_NoSubtract_Unbiased
│       └── Min-Max                            # Config. #6
└── Entity_Ranker
    ├── AllInteractions
    ├── complete.entity_ranking.run            # Full Complete, unfiltered neural entity ranking
    ├── filtered.entity_ranking.run            # Full Biased, filtered neural entity ranking
    ├── NoSubtract_Complete
    │   └── Min-Max                            # Input for Config. #6 (Unbiased)
    └── NoSubtract_Filtered
        └── Min-Max                            # Input for Config. #5 (Biased)
```

### Model Checkpoints
We provide the PyTorch model weights for both the **Document Re-ranker** and the **BERT-based Entity Ranker**.
Consistent with our evaluation data, checkpoints are provided for each of five folds, allowing for the reconstruction of our final rankings.

_(Note: Although we provide the ranking and model checkpoints for the **All-Interactions** entity ranker, the **No-Subtract** variant is the primary model utilized for the experimental configurations described in our study)_

_(Also note: While the configuration files for the **No-Subtract** checkpoints may contain references to a "subtract" interaction, this is a labeling artifact)_

#### Structure
```bash
Models
├── Document_Reranker
│   ├── BM25_AllInteractions          # Config. #1 & #3
│   │   ├── fold-0
│   │   ├── fold-1
│   │   ├── fold-2
│   │   ├── fold-3
│   │   └── fold-4
│   ├── BM25_NoSubtract               # Config. #2 & #4
│   ├── NEUR_AllInteractions_Biased
│   ├── NEUR_NoSubtract_Biased
│   │   └── Min-Max                   # Config. #5
│   └── NEUR_NoSubtract_Unbiased
│       └── Min-Max                   # Config. #6
└── Entity_Ranker
    ├── AllInteractions
    └── NoSubtract                    # Neural Entity Ranker (used for Configs. #5 & #6)
```